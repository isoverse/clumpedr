---
title: "Using clumpedr: basic data analysis"
author: "Ilja J. Kocken"
date: 2024-05-03
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using clumpedr: basic data analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Once you have `clumpedr` installed (see the README), you can first load the libraries:

# load the packages that we use
```{r setup}
  # library(tidyverse)  # a few of the below and many more
  library(glue)      # optional, if you want to glue strings together
  library(dplyr)     # for pipes, mutate, summarise, etc.
  library(tidyr)     # for nesting/unnesting
  library(ggplot2)   # for plots!

  library(isoreader) # read in raw data
  library(clumpedr)  # this package! process clumped isotope data
```

# get your data from raw instrument data into R

## load data from a remote

First locate where you have your raw data files stored.

Here I show how I load data from a remote Windows samba server on my GNU/Linux
machine.

Of course you can also just copy your files and paste them in a folder you
desire.

```{r, eval = FALSE}
folderstr <- "/run/user/1000/gvfs/smb-share:server=geofile02.geo.uu.nl,share=geo-labs/@RawData"
# read all did files
dids <- iso_read_dual_inlet(glue("{folderstr}/253pluskiel/Raw Data/Kiel Raw Data"),
                            cache = FALSE,
                            discard_duplicates = FALSE,
                            parallel = TRUE)
```

It is nice to save a cache/backup as an R data structure file, which we can read
in much faster.

```{r, eval = FALSE}
iso_save(dids, "out/dids.di.rds")
```

## load from the cache

Once we have saved the r data storage (`.rds`) file, we can load it much faster
than the raw data.

```{r, eval = FALSE}
dids  <- iso_read_dual_inlet("out/dids.di.rds")
```

I have made some standard data available here so as to run the tests, or a single
did file for an ETH-3 standard.

See their documentation with the following code:
```{r, eval = FALSE}
?standards
?eth3
```

# process the data!

We save the file info separately, since we will have to refer to it for some plots.

```{r}
stdinfo <- iso_get_file_info(standards)
glimpse(stdinfo)
```

This would also be the place to add potential fixes to typos in the file info
using `isoreader::iso_mutate_file_info()`.

## run the processing step-by-step

Note that normally, it is faster and smarter not to save the output of every
step as a separate tibble, but in this case we do it so that we can easily
inspect the results along the way. See the end of the vignette for the single
pipeline.

### filter measurements of interest

First we filter out the measurements we want, based on the Method name.

We use a regular expression, or `regexp`. They are very useful ways of looking
for patterns in strings.

Note that I put the name of the newly generated object at the end here for this
and future code chunks, so we `print()` the result for inspection.

```{r}
filt <- standards |>
  # we can subset to some files of interest (e.g., based on a regular expression)
  # in this case we subset to all the runs that have a "Clumped" method.
  iso_filter_files(grepl("Clumped.*met", Method))
filt
```

### extract raw data

Then we extract the raw data. This gives it in a format where each row
is one cycle of either `standard` or `sample` gas, with initensities
44--49 as columns.

```{r}
rawd <- filt |>
  # get all the raw data, per cycle from the dids
  iso_get_raw_data(include_file_info = "Analysis")
rawd
```

### disable failed cycles

This disables any cycles that have a sudden drop in pressure/intensity.

```{r}
disc <- rawd |>
  mutate(dis_min = 500, dis_max = 50000, dis_fac = 3) |>
  find_bad_cycles(min = "dis_min", max = "dis_max",
                  fac = "dis_fac", relative_to = "init")
disc |> select(file_id, outlier_cycle_low:outlier_cycle)
```

### find initial intensities of each cycle

```{r}
inits <- get_inits(rawd)
inits
```

### background correction

Do a very simple background correction based on the half-cup mass 54.

```{r}
bgds <- disc |>
  correct_backgrounds(factor = 0.82)
bgds |> select(file_id, v47.mV, v54.mV)
```

This overwrites the `v47.mV` column by subtracting `factor` * the `v54.mV`
column from `v47.mV`.

For a background correction based on background scans performed before each run
we have to get the raw scan data into R.

We can do this with:

```{r, eval = FALSE}
  scns <- iso_read_scan("scan_file.scn", cache = TRUE, parallel = TRUE, quiet = FALSE)
```

Processing the background scan files in this way is beyond the scope of this
vignette for now, since the functions that do the work are not generalized
enough to work for other set-ups.

See our
[clumped-processing](https://github.com/UtrechtUniversity/clumped-processing)
scripts for how we do background corrections and implement the full clumped
isotope workflow at Utrecht University.

### spread match

First we re-order the data into a wide format, where sample and reference gas
intensities are listed next to each other as separate columns using the
gather-unite-spread approach.

Then we compare reference gas to sample gas. With the `method="normal"`, this
would calculate the average of first and second cycles for the reference gas.
We can also use a work-in-progress linear interpolation (`method="linterp"`) to
match the mass 44 intensity of the reference gas to that of the sample gas and
apply this same shift to all the other masses. At present, it performs more
poorly than the regular calculation though, probably due to cycle elimination.

For example, we convert from the below:

| `file_id`      | type         | cycle | v44.mV | v45.mV | v46.mV | v47.mV | v54.mV |
|----------------|--------------|-------|--------|--------|--------|--------|--------|
| `"file_1.did"` | `"sample"`   | 1     | 1200   | 1100   | 1000   | 5000   | -302   |
| ...            | ...          | ...   | ...    | ...    | ...    | ...    | ...    |
| `"file_1.did"` | `"standard"` | 0     | 1300   | 1100   | 1000   | 5000   | -260   |
| `"file_1.did"` | `"standard"` | 1     | 1200   | 1120   | 1020   | 5020   | -230   |

to the following output:

| `file_id`      | `file_datetime`     | cycle | s44  | s45  | s46  | s47  | s54  | r44  | r45  | r46  | r47  | r54  |
|----------------|---------------------|-------|------|------|------|------|------|------|------|------|------|------|
| `"file_1.did"` | 2019-03-01 12:00:00 | 1     | 1200 | 1100 | 1000 | 5000 | -302 | 1250 | 1110 | 1010 | 5010 | -245 |

```{r}
sprd <- bgds |>
  spread_match(method = "normal")
sprd |> select(file_id, r44:s49)
```

### extract reference gas d13C and d18O values

```{r}
refd <- sprd |>
  append_ref_deltas(standards)
refd |> select(file_id, d13C_PDB_wg:d18O_PDBCO2_wg)
```
### calculate little delta's and clumped values

Now we can go to the bread and butter of clumpedr, delta calculations!

```{r}
abd <- refd |>
  abundance_ratios(i44 = s44, i45 = s45, i46 = s46, i47 = s47, i48 = s48, i49 = s49) |>
  abundance_ratios(i44 = r44, i45 = r45, i46 = r46, i47 = r47, i48 = r48, i49 = r49,
                   R45 = R45_wg, R46 = R46_wg, R47 = R47_wg, R48 = R48_wg, R49 = R49_wg)
abd |> select(file_id, R45:R49_wg)
```

```{r}
dlts <- abd |>
  little_deltas()
# this contains some more columns, but just showing the ones of interest for now
dlts |> select(file_id, d45:d49)
```

```{r}
bigD <- dlts |>
  mutate(Mineralogy = "Calcite") |>
  bulk_and_clumping_deltas()
  # outlier on the cycle level now contains all the reasons for cycle outliers
# it calculates more columns, but we show some of the new ones here:
bigD |> select(file_id, R13_wg:R17, C12, C626, C628, C828, # some columns not shown here
               d18O_PDB, d13C_PDB, R47_stoch, D47_raw)
```

### collapse cycles: calculate averages and standard deviations

```{r}
coll <- bigD |>
  collapse_cycles(cols = c(d13C_PDB, d18O_PDB, D47_raw), id = c(file_id, Analysis))
  # in the future we may want to use this nicer nesting approach?
  # It doesn't calculate summaries yet though.
  # nest_cycle_data(bgs = NULL)
coll |> select(file_id, d13C_PDB_mean, D47_raw_mean, D47_raw_cl)
```

### append metadata

This just `left_join()`s the metadata based on `file_id`, so that we can use it
for outlier removal etc.

```{r}
dati <- coll |>
  add_info(stdinfo, cols = c("file_root", "file_path", "file_subpath",
                             "file_datetime", "file_size", "Row",
                             "Peak Center", "Background", "Pressadjust",
                             "Reference Refill", "Line", "Sample",
                             "Weight [mg]", "Identifier 1", "Comment",
                             "Preparation", "Method", "measurement_info",
                             "MS_integration_time.s")) |>
  add_info(inits, cols = c("s44_init", "r44_init"))
dati |> select(file_id, file_root:r44_init)
```

### remove outliers

Based on several criteria, we can get rid of outliers. This needs to happen
before the Empirical Reference Frame is calculated and applied.

Here we just filter outliers based on initial intensities.

```{r}
rout <- dati |>
  unnest(cols = cycle_data) |>
  find_init_outliers(init_low = 4000, init_high = 40000, init_diff = 3000) |>
  summarize_outlier()
rout |> select(file_id, starts_with("outlier"))
```

### empirical transfer function

```{r}
detf <- rout |>
  append_expected_values(std_names = c("ETH-1", "ETH-2", "ETH-3"),
                         # I-CDES values (make sure to double-check which ones you use!)
                         std_values = c(0.2052, 0.2085, 0.6132)) |>
  calculate_etf() |>
  apply_etf()
  ## or the three functions above combined into one function
  ## empirical_transfer_function()
detf |> select(file_id, expected_D47:D47_etf)
```

### temperature calculation
```{r}
temp <- detf |>
  mutate(slope = 0.0397, intercept = 0.1518) |>
  temperature_calculation(D47 = D47_etf, slope = "slope", intercept = "intercept")
temp |> select(file_id, D47_raw_mean, D47_etf, temperature)
```

Note that in the case of many small replicates, it is better to do your
analysis based on the D47 values and then convert to temperature in the final
phase of your analysis, rather than here at the aliquot level.

Furthermore, error propagation of the calibration uncertainty is not incorporated here.
If you would like to include temperature uncertainty using bootstrapping, see
the package `clumpedcalib` on <https://github.com/japhir/clumpedcalib>.

## plots and summary

We can create some summary plots, i.e. of the final D47 values for each standard:

```{r, fig.width = 7, fig.height = 5}
# create a tibble that holds heights for text annotations
summ <- temp |>
  group_by(`Identifier 1`) |>
  summarise(y = max(D47_etf, na.rm = TRUE) + .05, n = n())

temp |>
  ggplot(aes(x = `Identifier 1`, y = D47_etf)) +
  geom_violin(aes(group = `Identifier 1`, fill = `Identifier 1`), alpha = 0.2) +
  geom_jitter(aes(colour = `Identifier 1`), width = .05, alpha = .6) +
  geom_text(aes(x = `Identifier 1`, y = y, label = n), data = summ, inherit.aes = FALSE)
```

If you do not understand any of the steps, look at the function documentation
(e.g.: `?empirical_transfer_function`) or look at the source code (type the
function name without parentheses into the command line).

Enjoy!

Here are all the steps in one pipeline:

```{r}
stdinfo <- iso_get_file_info(standards)

inits <- get_inits(iso_get_raw_data(standards, include_file_info = "Analysis"))

standards |>
  iso_filter_files(grepl("Clumped.*met", Method)) |>
  iso_get_raw_data(include_file_info = "Analysis") |>
  mutate(dis_min = 500, dis_max = 50000, dis_fac = 3) |>
  find_bad_cycles(min = "dis_min", max = "dis_max",
                  fac = "dis_fac", relative_to = "init") |>
  correct_backgrounds(0.82) |>
  spread_match(method = "normal") |>
  append_ref_deltas(standards) |>
  abundance_ratios(i44 = s44, i45 = s45, i46 = s46, i47 = s47, i48 = s48, i49 = s49) |>
  abundance_ratios(i44 = r44, i45 = r45, i46 = r46, i47 = r47, i48 = r48, i49 = r49,
                   R45 = R45_wg, R46 = R46_wg, R47 = R47_wg, R48 = R48_wg, R49 = R49_wg) |>
  little_deltas() |>
  mutate(Mineralogy = "Calcite") |>
  bulk_and_clumping_deltas() |>
  # outlier on the cycle level now contains all the reasons for cycle outliers
  summarise_outlier(quiet = TRUE) |>
  collapse_cycles(cols = c(d18O_PDBCO2, d13C_PDB, D47_raw), id = c(file_id, Analysis)) |>
  add_info(stdinfo, cols = c("file_root", "file_path", "file_subpath",
                             "file_datetime", "file_size", "Row",
                             "Peak Center", "Background", "Pressadjust",
                             "Reference Refill", "Line", "Sample",
                             "Weight [mg]", "Identifier 1", "Comment",
                             "Preparation", "Method", "measurement_info",
                             "MS_integration_time.s")) |>
  add_info(inits, cols = c("s44_init", "r44_init")) |>
  unnest(cols = cycle_data) |>
  find_init_outliers(init_low = 4000, init_high = 40000, init_diff = 3000) |>
  summarize_outlier() |>
  empirical_transfer_function() |>
  mutate(slope = 0.0397, intercept = 0.1518) |>
  temperature_calculation(D47 = D47_etf, slope = "slope", intercept = "intercept")
```

### HINT: look at plots interactively

Note that it is very nice to look at this plot---or any of the future
ones---interactively using `ggplotly`:

```{r, eval = FALSE}
plotly::toWebGL(plotly::ggplotly(dynamicTicks = TRUE))
```

This creates an interactive version of the last plot in your browser.

You can also assign extra aesthetics that you do not directly see to your plot,
for example `aes(ID = file_id, no = Analysis)` and this information will show
up when you hover a point.

Note that we use the `toWebGL` wrapper to make it run smoother for plots with
many points.
