---
title: "Using clumpedr: basic data analysis"
author: "Ilja J. Kocken"
date: 2019-11-13
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using clumpedr: basic data analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  ## comment = "#>"
  error = TRUE
)
```

Once you have `clumpedr` installed (see the README), you can first load the libraries:

# load the packages that we use
```{r}
  # library(tidyverse)  # a few of the below and many more
  library(glue)      # optional, if you want to glue strings together
  library(dplyr)     # for pipes, mutate, summarise, etc.
  library(tidyr)     # for nesting/unnesting
  library(ggplot2)   # for plots!

  library(isoreader) # read in raw data
  library(clumpedr)  # this package! process clumped isotope data
```

# get your data from raw instrument data into R

## load data from a remote

First locate where you have your raw data files stored.

Here I show how I load data from a remote Windows samba server on my GNU/Linux
machine.

Of course you can also just copy your files and paste them in a folder you
desire.

```{r, eval = FALSE}
folderstr <- "/run/user/1000/gvfs/smb-share:server=geofile02.geo.uu.nl,share=geo-labs/@RawData"
# read all did files
dids <- iso_read_dual_inlet(glue("{folderstr}/253pluskiel/Raw Data/Kiel Raw Data"),
                            cache = FALSE,
                            discard_duplicates = FALSE,
                            parallel = TRUE)
```

It is nice to save a cache/backup as an R data structure file, which we can read
in much faster.

```{r, eval = FALSE}
iso_save(dids, "out/dids.di.rds")
```

## load from the cache

Once we have saved the r data storage (`.rds`) file, we can load it much faster
than the raw data.

```{r, eval = FALSE}
dids  <- iso_read_dual_inlet("out/dids.di.rds")
```

I have made some standard data available here so as to run the tests, or a single
did file for an ETH-3 standard.

See their documentation with the following code:
```{r, eval = FALSE}
?standards
?eth3
```

# process the data!

We save the file info separately, since we will have to refer to it for some plots.

```{r}
stdinfo <- iso_get_file_info(standards)
```

This would also be the place to add potential fixes to typos in the file info
using `isoreader::iso_mutate_file_info()`.

## run the processing step-by-step

Note that normally, it is faster and smarter not to save the output of every
step as a separate tibble, but in this case we do it so that we can easily
inspect the results along the way. See the end of the vignette for the single
pipe.

### filter measurements of interest

First we filter out the measurements we want, based on the Method name.

We use a regular expression, or `regexp`. They are very useful ways of looking
for patterns in strings.

```{r}
filt <- standards |>
  # we can subset to some files of interest (e.g., based on a regular expression)
  # in this case we subset to all the runs that have a "Clumped" method.
  iso_filter_files(grepl("Clumped.*met", Method))
```

### extract raw data

Then we extract the raw data. This gives it in a format where each row
is one cycle of either `standard` or `sample` gas, with initensities
44--49 as columns.

```{r}
rawd <- filt |>
  # get all the raw data, per cycle from the dids
  iso_get_raw_data(include_file_info = "Analysis")
```

### disable failed cycles

This disables any cycles that have a sudden drop in pressure/intensity.

```{r}
disc <- rawd |>
  mutate(dis_min = 500, dis_max = 50000, dis_fac = 3) |>
  find_bad_cycles(min = dis_min, max = dis_max,
                  fac = dis_fac, relative_to = "init")
```

### find initial intensities of each cycle

```{r}
inits <- get_inits(rawd)
```

### background correction

Do a very simple background correction based on the half-cup mass 54.

```{r}
bgds <- disc |>
  correct_backgrounds(factor = 0.82)
```

For a background correction based on background scans performed before each run
we have to get the raw scan data into R.

We can do this with:

```{r, eval = FALSE}
  scns <- iso_read_scan("scan_file.scn", cache = TRUE, parallel = TRUE, quiet = FALSE)
```

Processing the background scan files in this way is beyond the scope of this
vignette for now, since the functions that do the work are not generalized
enough to work for other set-ups.

### spread match

First we re-order the data into a wide format, where sample and reference gas
intensities are listed next to each other as separate columns using the
gather-unite-spread approach.

Then we compare reference gas to sample gas. With the `method="normal"`, this
would calculate the average of first and second cycles for the reference gas.
We can also use a work-in-progress linear interpolation (`method="linterp"`) to
match the mass 44 intensity of the reference gas to that of the sample gas and
apply this same shift to all the other masses. At present, it performs more
poorly than the regular calculation though, probably due to cycle elimination.

For example, we convert from the below:

| `file_id`      | type         | cycle | v44.mV | v45.mV | v46.mV | v47.mV | v54.mV |
|----------------|--------------|-------|--------|--------|--------|--------|--------|
| `"file_1.did"` | `"sample"`   | 1     | 1200   | 1100   | 1000   | 5000   | -302   |
| ...            | ...          | ...   | ...    | ...    | ...    | ...    | ...    |
| `"file_1.did"` | `"standard"` | 0     | 1300   | 1100   | 1000   | 5000   | -260   |
| `"file_1.did"` | `"standard"` | 1     | 1200   | 1120   | 1020   | 5020   | -230   |

to the following output:

| `file_id`      | `file_datetime`     | cycle | s44  | s45  | s46  | s47  | s54  | r44  | r45  | r46  | r47  | r54  |
|----------------|---------------------|-------|------|------|------|------|------|------|------|------|------|------|
| `"file_1.did"` | 2019-03-01 12:00:00 | 1     | 1200 | 1100 | 1000 | 5000 | -302 | 1250 | 1110 | 1010 | 5010 | -245 |

```{r}
sprd <- disc |>
  spread_match(method = "normal")
```

### extract reference gas d13C and d18O values

```{r}
refd <- sprd |>
  append_ref_deltas(standards)
```
### calculate little delta's and clumped values

Now we can go to the bread and butter of clumpedr, delta calculations!

```{r}
dlts <- refd |>
  abundance_ratios(s44, s45, s46, s47, s48, s49) |>
  abundance_ratios(r44, r45, r46, r47, r48, r49,
                   R45_wg, R46_wg, R47_wg, R48_wg, R49_wg) |>
  little_deltas() |>
  mutate(Mineralogy = "Calcite", R18_PDB = clumpedr:::default(R18_PDB)) |>
  bulk_and_clumping_deltas(R18_PDB = .data$R18_PDB) |>
  # outlier on the cycle level now contains all the reasons for cycle outliers
  summarise_outlier(quiet = TRUE)
```

### collapse cycles: calculate averages and standard deviations

```{r}
coll <- dlts |>
  clumpedr:::collapse_cycles(cols = c(d13C_PDB, d18O_PDB, D47_raw), id = c(file_id, Analysis)) # this is an older function which we will likely deprecate in the future in favour of:
  ## nest_cycle_data()
```

### append metadata

This just `left_join`s the metadata based on `file_id`, so that we can use it
for outlier removal etc.

```{r}
dati <- coll |>
  add_info(stdinfo, cols = c("file_root", "file_path", "file_subpath",
                             "file_datetime", "file_size", "Row",
                             "Peak Center", "Background", "Pressadjust",
                             "Reference Refill", "Line", "Sample",
                             "Weight [mg]", "Identifier 1", "Comment",
                             "Preparation", "Method", "measurement_info",
                             "MS_integration_time.s")) |>
  add_info(inits, cols = c("s44_init", "r44_init"))
```

### remove outliers

Based on several criteria, we can get rid of outliers. This needs to happen
before the Empirical Reference Frame is calculated and applied.

Here we just filter outliers based on initial intensities.

```{r}
rout <- dati |>
  unnest(cols = cycle_data) |>
  find_init_outliers(init_low = 4000, init_high = 40000, init_diff = 3000)
```

### empirical transfer function

```{r}
detf <- rout |>
  empirical_transfer_function()
```

### temperature calculation
```{r}
temp <- detf |>
  temperature_calculation(D47 = D47_etf, slope = 0.0397, intercept = 0.1518)
```

Note that in the case of many small replicates, it is better to do your analysis based on the D47 in stead of on aliquot temperatures. Furthermore, error propagation of the calibration uncertainty is not incorporated here.

## plots and summary

We can create some summary plots, i.e. of the temperature per standard:

```{r}
# create a tibble that holds heights for text annotations
summ <- temp |>
  group_by(`Identifier 1`) |>
  summarise(y = max(D47_etf, na.rm = TRUE) + .05, n = n())

temp |>
  ggplot(aes(x = `Identifier 1`, y = D47_etf)) +
  geom_violin(aes(group = `Identifier 1`, fill = `Identifier 1`), alpha = 0.2) +
  geom_jitter(width = .05, alpha = .6) +
  geom_text(aes(x = `Identifier 1`, y = y, label = n), data = summ, inherit.aes = FALSE)
```

If you do not understand any of the steps, look at the function documentation
(e.g.: `?empirical_transfer_function`) or look at the source code (type the
function name without parentheses into the command line).

Enjoy!

Here are all the steps in one pipeline:

```{r}
stdinfo <- iso_get_file_info(standards)

inits <- get_inits(iso_get_raw_data(standards, include_file_info = "Analysis"))

standards |>
  iso_filter_files(grepl("Clumped.*met", Method)) |>
  iso_get_raw_data(include_file_info = "Analysis") |>
  mutate(dis_min = 500, dis_max = 50000, dis_fac = 3) |>
  find_bad_cycles(min = "dis_min", max = "dis_max",
                  fac = "dis_fac", relative_to = "init") |>
  correct_backgrounds(0.82) |>
  spread_match(method = "normal") |>
  append_ref_deltas(standards) |>
  abundance_ratios(s44, s45, s46, s47, s48, s49) |>
  abundance_ratios(r44, r45, r46, r47, r48, r49,
                   R45_wg, R46_wg, R47_wg, R48_wg, R49_wg) |>
  little_deltas() |>
  mutate(Mineralogy = "Calcite", R18_PDB = clumpedr:::default(R18_PDB)) |>
  bulk_and_clumping_deltas(R18_PDB = .data$R18_PDB) |>
  # outlier on the cycle level now contains all the reasons for cycle outliers
  summarise_outlier(quiet = TRUE) |>
  collapse_cycles(cols = c(d18O_PDBCO2, d13C_PDB, D47_raw), id = c(file_id, Analysis)) |>
  add_info(stdinfo, cols = c("file_root", "file_path", "file_subpath",
                             "file_datetime", "file_size", "Row",
                             "Peak Center", "Background", "Pressadjust",
                             "Reference Refill", "Line", "Sample",
                             "Weight [mg]", "Identifier 1", "Comment",
                             "Preparation", "Method", "measurement_info",
                             "MS_integration_time.s")) |>
  add_info(inits, cols = c("s44_init", "r44_init")) |>
  unnest(cols = cycle_data) |>
  find_init_outliers(init_low = 4000, init_high = 40000, init_diff = 3000) |>
  empirical_transfer_function() |>
  temperature_calculation(slope = 0.0397, intercept = 0.1518)
```

### HINT: look at plots interactively

Note that it is very nice to look at this plot---or any of the future
ones---interactively using `ggplotly`:

```{r, eval = FALSE}
plotly::toWebGL(plotly::ggplotly(dynamicTicks = TRUE))
```

This creates an interactive version of the last plot in your browser.

You can also assign extra aesthetics that you do not directly see to your plot,
for example `aes(ID = file_id, no = Analysis)` and this information will show
up when you hover a point.

Note that we use the `toWebGL` wrapper to make it run smoother for plots with
many points.
